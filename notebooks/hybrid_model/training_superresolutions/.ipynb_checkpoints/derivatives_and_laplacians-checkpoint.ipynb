{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to velocities, 1st derivatives and laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_cfd.ml.diego_cnn_bcs import *\n",
    "\n",
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jax_cfd.base as cfd\n",
    "from jax_cfd.ml import towers\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "from jax_cfd.base import finite_differences as fd\n",
    "from jax_cfd.base import grids\n",
    "\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import xarray\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "# from jax_cfd.ml.diego_model_utils import SaveObject, forward_pass_module\n",
    "from jax_cfd.ml.diego_preprocessing import *\n",
    "from jax_cfd.ml.diego_train_functions import *\n",
    "from jax_cfd.ml import nonlinearities\n",
    "\n",
    "from jax_cfd.ml.newSaveObject import *\n",
    "from jax_cfd.ml.diego_towers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (fine grid)\n",
    "# create X_data via mean pooling\n",
    "# create Y_data by calculating everything for each frame and stacking them along the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '256x64_150_seconds_inner_1'\n",
    "data = xarray.open_dataset(f'../../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "high_def = []\n",
    "for i in range(int(len(data.time))):\n",
    "    this_time = np.dstack([\n",
    "        jnp.array([data.u.isel(time = i)]).reshape(x_shape,y_shape).T,\n",
    "        jnp.array([data.v.isel(time = i)]).reshape(x_shape,y_shape).T\n",
    "    ])\n",
    "    high_def.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: \t\t0.015625\n",
      "outer_steps: \t9600\n",
      "inner_steps: \t1.0\n",
      "total_sim_time: 150.0\n",
      "removed points: 960\n",
      "\n",
      "\n",
      "step = 100\n",
      "Training dataset shape: \n",
      "\t(87, 64, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "#warm up time (may want to discard initial stages of simulation since not really representative of turbulent flow?)\n",
    "dt = float(data.time[0].values)\n",
    "\n",
    "outer_steps = len(data.time.values)\n",
    "\n",
    "inner_steps = (data.time[1].values-data.time[0].values)/dt\n",
    "\n",
    "total_sim_time = outer_steps*inner_steps*dt\n",
    "print(\"dt: \\t\\t\" + str(dt))\n",
    "print(\"outer_steps: \\t\" + str(outer_steps))\n",
    "print(\"inner_steps: \\t\" + str(inner_steps))\n",
    "print(\"total_sim_time: \" + str(total_sim_time))\n",
    "\n",
    "warm_up = 15 #seconds\n",
    "warm_index = int(warm_up/total_sim_time * outer_steps // 1)\n",
    "print(\"removed points: \" + str(warm_index))\n",
    "high_def = high_def[warm_index:]\n",
    "\n",
    "print(\"\\n\")\n",
    "step = 100\n",
    "high_def = high_def[0::step]\n",
    "print(\"step = \" + str(step))\n",
    "print(\"Training dataset shape: \") # (frames, x, y, input channels)\n",
    "print(\"\\t\" + str(np.shape(high_def)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.4 ms, sys: 970 µs, total: 10.4 ms\n",
      "Wall time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time high_def_norm,ogMean,ogStdDev = normalisingDataset(high_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining what we are training to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_outputs = {\n",
    "        \"u\" : False,\n",
    "        \"du\" : True,\n",
    "        \"lapu\" : True,\n",
    "        \n",
    "        \"v\" : False,\n",
    "        \"dv\" : True,\n",
    "        \"lapv\" : True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create X dataset: \n",
      "CPU times: user 1.94 s, sys: 621 µs, total: 1.94 s\n",
      "Wall time: 1.95 s\n",
      "\n",
      "Create Y dataset: \n",
      "CPU times: user 180 ms, sys: 0 ns, total: 180 ms\n",
      "Wall time: 181 ms\n",
      "\n",
      "Padding all datasets: \n",
      "CPU times: user 1.59 s, sys: 7.89 ms, total: 1.6 s\n",
      "Wall time: 1.61 s\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/FYP/forked_jax/jax-cfd/jax_cfd/ml/diego_cnn_bcs.py:191\u001b[0m, in \u001b[0;36mpadYDataset\u001b[0;34m(dataset, padding, which_outputs, conditions)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     pre_out\u001b[38;5;241m.\u001b[39mappend(channelFlowPadding(temp[\u001b[38;5;241m5\u001b[39m],padding,dvdxT,dvdxB)) \u001b[38;5;66;03m# dvdx\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     pre_out\u001b[38;5;241m.\u001b[39mappend(channelFlowPadding(\u001b[43mtemp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m,padding,dvdyT,dvdyB)) \u001b[38;5;66;03m# dvdy\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlapv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     pre_out\u001b[38;5;241m.\u001b[39mappend(channelFlowPadding(temp[\u001b[38;5;241m7\u001b[39m],padding,lapVT,lapVB)) \u001b[38;5;66;03m# lap(v)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 363 ms, sys: 3.91 ms, total: 366 ms\n",
      "Wall time: 368 ms\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/FYP/forked_jax/jax-cfd/jax_cfd/ml/diego_cnn_bcs.py:191\u001b[0m, in \u001b[0;36mpadYDataset\u001b[0;34m(dataset, padding, which_outputs, conditions)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     pre_out\u001b[38;5;241m.\u001b[39mappend(channelFlowPadding(temp[\u001b[38;5;241m5\u001b[39m],padding,dvdxT,dvdxB)) \u001b[38;5;66;03m# dvdx\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     pre_out\u001b[38;5;241m.\u001b[39mappend(channelFlowPadding(\u001b[43mtemp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m,padding,dvdyT,dvdyB)) \u001b[38;5;66;03m# dvdy\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlapv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     pre_out\u001b[38;5;241m.\u001b[39mappend(channelFlowPadding(temp[\u001b[38;5;241m7\u001b[39m],padding,lapVT,lapVB)) \u001b[38;5;66;03m# lap(v)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes of all datasets\n",
      "(69, 18, 66, 2)\n",
      "(69, 4, 16, 6)\n",
      "(18, 18, 66, 2)\n",
      "(18, 4, 16, 6)\n"
     ]
    }
   ],
   "source": [
    "#split into train and test\n",
    "\n",
    "split = 0.8\n",
    "split = int(len(high_def)*split//1)\n",
    "random.shuffle(high_def)\n",
    "\n",
    "factor = 4\n",
    "\n",
    "print(\"Create X dataset: \")\n",
    "%time X_dataset = creatingDataset(high_def_norm,mean_pooling,factor)\n",
    "\n",
    "print(\"\\nCreate Y dataset: \")\n",
    "%time Y_dataset = createDatasetDerivatives(high_def_norm,sampling,factor,which_outputs)\n",
    "\n",
    "# %time Y_dataset = calculateResiduals(X_dataset,Y_dataset)\n",
    "\n",
    "\n",
    "X_train = X_dataset[:split]\n",
    "Y_train = Y_dataset[:split]\n",
    "\n",
    "X_test = X_dataset[split:]\n",
    "Y_test = Y_dataset[split:]\n",
    "\n",
    "# NOTE: padding conditions can be specified via the \"conditions\" input to the padYDataset function below\n",
    "# the padXDataset only pads u and v for channel flow conditions, so hard coded for impermeability and no-slip conditions (0,0)\n",
    "print(\"\\nPadding all datasets: \")\n",
    "padding = [1,1] #for a 3 by 3 kernel, find a better way to define this (so not redifined when creating CNN)\n",
    "%time X_train = padXDataset(X_train,padding)\n",
    "%time Y_train = padYDataset(Y_train,padding)\n",
    "\n",
    "%time X_test = padXDataset(X_test,padding)\n",
    "%time Y_test = padYDataset(Y_test,padding)\n",
    "\n",
    "print(\"\\nShapes of all datasets\")\n",
    "printAllShapes(X_train,Y_train, X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = [1,1] # for a 3 by 3 kernel\n",
    "thing = createPaddedMesh(high_def[1][:,:,1],padding)\n",
    "thing = channelFlowPadding(thing,padding,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the derivatives and laplacians have already been calculated, only need to specify bcs for each of the ones we are using\n",
    "conditions = [\n",
    "    [0,0],#u\n",
    "    [0,0],#dudx\n",
    "    [0,0],#dudy\n",
    "    [0,0],#lapu\n",
    "    \n",
    "    [0,0],#v\n",
    "    [0,0],#dvdx\n",
    "    [0,0],#dvdy\n",
    "    [0,0]#lapv\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmykeys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmyvals\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "mykeys[myvals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 8\n",
    "# spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(x):\n",
    "    cnn = CNN(CNN_specs)\n",
    "    return cnn(x)\n",
    "\n",
    "CNN_specs = {\n",
    "    \"hidden_channels\" : 4,\n",
    "    \"hidden_layers\" : 2,\n",
    "    \"nonlinearity\" : \"relu\",\n",
    "    \"num_output_channels\" : 1\n",
    "}\n",
    "\n",
    "CNN_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "learning_rates = staggeredLearningRate((120,0.01),(70,0.001))\n",
    "printEvery=25\n",
    "%time losses,val_losses,params = train(X_train,Y_train,X_test,Y_test,rng_key,input_channels,epochs,printEvery=printEvery,learning_rates=learning_rates,params=None,forward_pass=forward_pass,tol = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "plt.plot(losses[::step], label=\"training\")\n",
    "plt.plot(val_losses[::step],label=\"validation\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.xlabel(\"epochs\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toSave = newSaveObject(params,CNN_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./pred_all_eight.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,\"wb\") as f:\n",
    "    pickle.dump(toSave,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'rb',) as pickle_file:\n",
    "    loaded = pickle.load(pickle_file)\n",
    "    CNN_specs = loaded.CNN_specs\n",
    "    loaded.forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 66, 8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(loaded.forward_pass.apply(loaded.params,X_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
