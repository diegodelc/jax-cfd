{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to velocities, 1st derivatives and laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_cfd.ml.diego_cnn_bcs import *\n",
    "\n",
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jax_cfd.base as cfd\n",
    "from jax_cfd.ml import towers\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "from jax_cfd.base import finite_differences as fd\n",
    "from jax_cfd.base import grids\n",
    "\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import xarray\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "# from jax_cfd.ml.diego_model_utils import SaveObject, forward_pass_module\n",
    "from jax_cfd.ml.diego_preprocessing import *\n",
    "from jax_cfd.ml.diego_train_functions import *\n",
    "from jax_cfd.ml import nonlinearities\n",
    "\n",
    "from jax_cfd.ml.newSaveObject import *\n",
    "from jax_cfd.ml.diego_towers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (fine grid)\n",
    "# create X_data via mean pooling\n",
    "# create Y_data by calculating everything for each frame and stacking them along the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '256x64_150_seconds_inner_1'\n",
    "data = xarray.open_dataset(f'../../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "high_def = []\n",
    "for i in range(int(len(data.time))):\n",
    "    this_time = np.dstack([\n",
    "        jnp.array([data.u.isel(time = i)]).reshape(x_shape,y_shape).T,\n",
    "        jnp.array([data.v.isel(time = i)]).reshape(x_shape,y_shape).T\n",
    "    ])\n",
    "    high_def.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: \t\t0.015625\n",
      "outer_steps: \t9600\n",
      "inner_steps: \t1.0\n",
      "total_sim_time: 150.0\n",
      "removed points: 960\n",
      "\n",
      "\n",
      "step = 10\n",
      "Training dataset shape: \n",
      "\t(864, 64, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "#warm up time (may want to discard initial stages of simulation since not really representative of turbulent flow?)\n",
    "dt = float(data.time[0].values)\n",
    "\n",
    "outer_steps = len(data.time.values)\n",
    "\n",
    "inner_steps = (data.time[1].values-data.time[0].values)/dt\n",
    "\n",
    "total_sim_time = outer_steps*inner_steps*dt\n",
    "print(\"dt: \\t\\t\" + str(dt))\n",
    "print(\"outer_steps: \\t\" + str(outer_steps))\n",
    "print(\"inner_steps: \\t\" + str(inner_steps))\n",
    "print(\"total_sim_time: \" + str(total_sim_time))\n",
    "\n",
    "warm_up = 15 #seconds\n",
    "warm_index = int(warm_up/total_sim_time * outer_steps // 1)\n",
    "print(\"removed points: \" + str(warm_index))\n",
    "high_def = high_def[warm_index:]\n",
    "\n",
    "print(\"\\n\")\n",
    "step = 10\n",
    "high_def = high_def[0::step]\n",
    "print(\"step = \" + str(step))\n",
    "print(\"Training dataset shape: \") # (frames, x, y, input channels)\n",
    "print(\"\\t\" + str(np.shape(high_def)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.2 ms, sys: 14 ms, total: 94.3 ms\n",
      "Wall time: 94.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time high_def_norm,ogMean,ogStdDev = normalisingDataset(high_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining what we are training towards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_outputs = {\n",
    "        \"u\" : False,\n",
    "        \"du\" : False,\n",
    "        \"lapu\" : True,\n",
    "        \n",
    "        \"v\" : False,\n",
    "        \"dv\" : False,\n",
    "        \"lapv\" : True\n",
    "    }\n",
    "\n",
    "conditions = [\n",
    "#             [0,0],#u\n",
    "#             [0,0],#dudx\n",
    "#             [0,0],#dudy\n",
    "            [0,0],#lapu\n",
    "\n",
    "#             [0,0],#v\n",
    "#             [0,0],#dvdx\n",
    "#             [0,0],#dvdy\n",
    "            [0,0]#lapv\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create X dataset: \n",
      "CPU times: user 19.3 s, sys: 19.4 ms, total: 19.3 s\n",
      "Wall time: 19.4 s\n",
      "\n",
      "Create Y dataset: \n",
      "CPU times: user 1.36 s, sys: 8.79 ms, total: 1.37 s\n",
      "Wall time: 1.37 s\n",
      "\n",
      "Padding all datasets: \n",
      "CPU times: user 13.9 s, sys: 9.58 ms, total: 13.9 s\n",
      "Wall time: 14 s\n",
      "CPU times: user 13.7 s, sys: 7.17 ms, total: 13.7 s\n",
      "Wall time: 13.8 s\n",
      "CPU times: user 3.43 s, sys: 3.64 ms, total: 3.43 s\n",
      "Wall time: 3.45 s\n",
      "CPU times: user 3.4 s, sys: 1.81 ms, total: 3.4 s\n",
      "Wall time: 3.42 s\n",
      "\n",
      "Shapes of all datasets\n",
      "(691, 18, 66, 2)\n",
      "(691, 18, 66, 2)\n",
      "(173, 18, 66, 2)\n",
      "(173, 18, 66, 2)\n"
     ]
    }
   ],
   "source": [
    "#split into train and test\n",
    "\n",
    "split = 0.8\n",
    "split = int(len(high_def)*split//1)\n",
    "random.shuffle(high_def)\n",
    "\n",
    "factor = 4\n",
    "\n",
    "print(\"Create X dataset: \")\n",
    "%time X_dataset = creatingDataset(high_def_norm,mean_pooling,factor)\n",
    "\n",
    "print(\"\\nCreate Y dataset: \")\n",
    "%time Y_dataset = createDatasetDerivatives(high_def_norm,sampling,factor,which_outputs)\n",
    "\n",
    "# %time Y_dataset = calculateResiduals(X_dataset,Y_dataset)\n",
    "\n",
    "\n",
    "X_train = X_dataset[:split]\n",
    "Y_train = Y_dataset[:split]\n",
    "\n",
    "X_test = X_dataset[split:]\n",
    "Y_test = Y_dataset[split:]\n",
    "\n",
    "# NOTE: padding conditions can be specified via the \"conditions\" input to the padYDataset function below\n",
    "# the padXDataset only pads u and v for channel flow conditions, so hard coded for impermeability and no-slip conditions (0,0)\n",
    "print(\"\\nPadding all datasets: \")\n",
    "padding = [1,1] #for a 3 by 3 kernel, find a better way to define this (so not redifined when creating CNN)\n",
    "%time X_train = padXDataset(X_train,padding)\n",
    "%time Y_train = padYDataset(Y_train,padding,conditions)\n",
    "\n",
    "%time X_test = padXDataset(X_test,padding)\n",
    "%time Y_test = padYDataset(Y_test,padding,conditions)\n",
    "\n",
    "print(\"\\nShapes of all datasets\")\n",
    "printAllShapes(X_train,Y_train, X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(x):\n",
    "    cnn = CNN(CNN_specs)\n",
    "    return cnn(x)\n",
    "\n",
    "CNN_specs = {\n",
    "    \"hidden_channels\" : 8,\n",
    "    \"hidden_layers\" : 6,\n",
    "    \"nonlinearity\" : \"relu\",\n",
    "    \"num_output_channels\" : 2\n",
    "}\n",
    "input_channels = 2\n",
    "\n",
    "# CNN_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of all datasets\n",
      "(691, 18, 66, 2)\n",
      "(691, 18, 66, 2)\n",
      "(173, 18, 66, 2)\n",
      "(173, 18, 66, 2)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "UpdateWeights() missing 1 required positional argument: 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/FYP/forked_jax/jax-cfd/jax_cfd/ml/diego_train_functions.py:131\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_train, Y_train, X_test, Y_test, rng_key, input_channels, epochs, learning_rates, update_weights, printEvery, params, forward_pass, tol)\u001b[0m\n\u001b[1;32m    122\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    124\u001b[0m     \n\u001b[1;32m    125\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m#TODO: using test as validation, change this!\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     params,loss,val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43mupdate_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39mprintEvery \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m#every n epochs\u001b[39;00m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i,epochs))\n",
      "File \u001b[0;32m~/FYP/forked_jax/jax-cfd/jax_cfd/ml/diego_train_functions.py:89\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(params, X_train, Y_train, X_val, Y_val, learning_rate, forward_pass, update_weights)\u001b[0m\n\u001b[1;32m     87\u001b[0m loss, param_grads \u001b[38;5;241m=\u001b[39m value_and_grad(MeanSquaredErrorLoss)(params, X_train, Y_train,forward_pass)\n\u001b[1;32m     88\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m MeanSquaredErrorLoss(params, X_val,Y_val,forward_pass)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grads\u001b[49m\u001b[43m)\u001b[49m, loss, val_loss\n",
      "File \u001b[0;32m~/anaconda3/envs/PNAS_codes/lib/python3.10/site-packages/jax/_src/tree_util.py:207\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    205\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    206\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PNAS_codes/lib/python3.10/site-packages/jax/_src/tree_util.py:207\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    206\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "\u001b[0;31mTypeError\u001b[0m: UpdateWeights() missing 1 required positional argument: 'learning_rate'"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "learning_rates = staggeredLearningRate((120,0.01),(70,0.001))\n",
    "printEvery=25\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "def UpdateWeights(weights,gradients):\n",
    "    return weights - learning_rate * gradients\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "%time losses,val_losses,params = train(X_train,Y_train,X_test,Y_test,rng_key,input_channels,epochs,update_weights=UpdateWeights,printEvery=printEvery,learning_rates=learning_rate,params=None,forward_pass=forward_pass,tol = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m[::step], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses[::step],label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "\n",
    "plt.plot(losses[::step], label=\"training\")\n",
    "plt.plot(val_losses[::step],label=\"validation\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.xlabel(\"epochs\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toSave = newSaveObject(params,CNN_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./../models/correctors/laplacian_correctors/first_laplacians.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,\"wb\") as f:\n",
    "    pickle.dump(toSave,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'rb',) as pickle_file:\n",
    "    loaded = pickle.load(pickle_file)\n",
    "    CNN_specs = loaded.CNN_specs\n",
    "    loaded.forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 66, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(loaded.forward_pass.apply(loaded.params,X_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
