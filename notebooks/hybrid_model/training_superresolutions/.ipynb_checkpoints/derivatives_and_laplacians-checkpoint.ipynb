{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to velocities, 1st derivatives and laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_cfd.ml.diego_cnn_bcs import *\n",
    "\n",
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jax_cfd.base as cfd\n",
    "from jax_cfd.ml import towers\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "from jax_cfd.base import finite_differences as fd\n",
    "from jax_cfd.base import grids\n",
    "\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import xarray\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "from jax_cfd.ml.diego_model_utils import SaveObject, forward_pass_module\n",
    "from jax_cfd.ml.diego_preprocessing import *\n",
    "from jax_cfd.ml.diego_train_functions import *\n",
    "from jax_cfd.ml import nonlinearities\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (fine grid)\n",
    "# create X_data via mean pooling\n",
    "# create Y_data by calculating everything for each frame and stacking them along the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '256x64_150_seconds_inner_1'\n",
    "data = xarray.open_dataset(f'../../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "high_def = []\n",
    "for i in range(int(len(data.time))):\n",
    "    this_time = np.dstack([\n",
    "        jnp.array([data.u.isel(time = i)]).reshape(x_shape,y_shape).T,\n",
    "        jnp.array([data.v.isel(time = i)]).reshape(x_shape,y_shape).T\n",
    "    ])\n",
    "    high_def.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: \t\t0.015625\n",
      "outer_steps: \t9600\n",
      "inner_steps: \t1.0\n",
      "total_sim_time: 150.0\n",
      "removed points: 960\n",
      "\n",
      "\n",
      "step = 50\n",
      "Training dataset shape: \n",
      "\t(173, 64, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "#warm up time (may want to discard initial stages of simulation since not really representative of turbulent flow?)\n",
    "dt = float(data.time[0].values)\n",
    "\n",
    "outer_steps = len(data.time.values)\n",
    "\n",
    "inner_steps = (data.time[1].values-data.time[0].values)/dt\n",
    "\n",
    "total_sim_time = outer_steps*inner_steps*dt\n",
    "print(\"dt: \\t\\t\" + str(dt))\n",
    "print(\"outer_steps: \\t\" + str(outer_steps))\n",
    "print(\"inner_steps: \\t\" + str(inner_steps))\n",
    "print(\"total_sim_time: \" + str(total_sim_time))\n",
    "\n",
    "warm_up = 15 #seconds\n",
    "warm_index = int(warm_up/total_sim_time * outer_steps // 1)\n",
    "print(\"removed points: \" + str(warm_index))\n",
    "high_def = high_def[warm_index:]\n",
    "\n",
    "print(\"\\n\")\n",
    "step = 50\n",
    "high_def = high_def[0::step]\n",
    "print(\"step = \" + str(step))\n",
    "print(\"Training dataset shape: \") # (frames, x, y, input channels)\n",
    "print(\"\\t\" + str(np.shape(high_def)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 ms, sys: 13.8 ms, total: 34.6 ms\n",
      "Wall time: 34.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time high_def_norm,ogMean,ogStdDev = normalisingDataset(high_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create X dataset: \n",
      "CPU times: user 3.75 s, sys: 3.39 ms, total: 3.76 s\n",
      "Wall time: 3.76 s\n",
      "\n",
      "Create Y dataset: \n",
      "CPU times: user 881 ms, sys: 7.66 ms, total: 889 ms\n",
      "Wall time: 891 ms\n",
      "\n",
      "Padding all datasets: \n",
      "CPU times: user 2.98 s, sys: 12.4 ms, total: 2.99 s\n",
      "Wall time: 3 s\n",
      "CPU times: user 10.7 s, sys: 23.8 ms, total: 10.7 s\n",
      "Wall time: 10.8 s\n",
      "CPU times: user 678 ms, sys: 1.72 ms, total: 679 ms\n",
      "Wall time: 682 ms\n",
      "CPU times: user 2.7 s, sys: 1.66 ms, total: 2.7 s\n",
      "Wall time: 2.72 s\n",
      "\n",
      "Shapes of all datasets\n",
      "(138, 18, 66, 2)\n",
      "(138, 18, 66, 8)\n",
      "(35, 18, 66, 2)\n",
      "(35, 18, 66, 8)\n"
     ]
    }
   ],
   "source": [
    "#split into train and test\n",
    "\n",
    "split = 0.8\n",
    "split = int(len(high_def)*split//1)\n",
    "random.shuffle(high_def)\n",
    "\n",
    "factor = 4\n",
    "\n",
    "print(\"Create X dataset: \")\n",
    "%time X_dataset = creatingDataset(high_def_norm,mean_pooling,factor)\n",
    "\n",
    "print(\"\\nCreate Y dataset: \")\n",
    "%time Y_dataset = createDatasetDerivatives(high_def_norm,sampling,factor)\n",
    "\n",
    "# %time Y_dataset = calculateResiduals(X_dataset,Y_dataset)\n",
    "\n",
    "\n",
    "X_train = X_dataset[:split]\n",
    "Y_train = Y_dataset[:split]\n",
    "\n",
    "X_test = X_dataset[split:]\n",
    "Y_test = Y_dataset[split:]\n",
    "\n",
    "# NOTE: padding conditions can be specified via the \"conditions\" input to the padYDataset function below\n",
    "# the padXDataset only pads u and v for channel flow conditions, so hard coded for impermeability and no-slip conditions (0,0)\n",
    "print(\"\\nPadding all datasets: \")\n",
    "padding = [1,1] #for a 3 by 3 kernel, find a better way to define this (so not redifined when creating CNN)\n",
    "%time X_train = padXDataset(X_train,padding)\n",
    "%time Y_train = padYDataset(Y_train,padding)\n",
    "\n",
    "%time X_test = padXDataset(X_test,padding)\n",
    "%time Y_test = padYDataset(Y_test,padding)\n",
    "\n",
    "print(\"\\nShapes of all datasets\")\n",
    "printAllShapes(X_train,Y_train, X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = [1,1] # for a 3 by 3 kernel\n",
    "thing = createPaddedMesh(high_def[1][:,:,1],padding)\n",
    "thing = channelFlowPadding(thing,padding,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 8\n",
    "# spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tower_module = towers.forward_tower_factory #add scaling here\n",
    "\n",
    "class CNN(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"CNN\")\n",
    "        components = []\n",
    "        \n",
    "        components.append(hk.Conv2D(output_channels=2*num_output_channels, kernel_shape=(3,3), padding=\"SAME\"))\n",
    "        components.append(nonlinearities.relu)\n",
    "        components.append(hk.Conv2D(output_channels=2*num_output_channels, kernel_shape=(3,3), padding=\"SAME\"))\n",
    "        components.append(nonlinearities.relu)\n",
    "        components.append(hk.Conv2D(output_channels=num_output_channels, kernel_shape=(3,3), padding=\"SAME\"))\n",
    "        components.append(nonlinearities.relu)\n",
    "        \n",
    "        self.components = components\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = hk.Sequential(self.components)(x)\n",
    "        return x\n",
    "\n",
    "def ConvNet(x):\n",
    "    cnn = CNN()\n",
    "    return cnn(x)\n",
    "\n",
    "forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of all datasets\n",
      "(138, 18, 66, 2)\n",
      "(138, 18, 66, 8)\n",
      "(35, 18, 66, 2)\n",
      "(35, 18, 66, 8)\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "\tmse : 0.216923\t\tval mse : 0.210407\tEstimated end time: 22:49:02\n",
      "\n",
      "\n",
      "Epoch 2/3\n",
      "\tmse : 0.199309\t\tval mse : 0.196374\tEstimated end time: 22:49:04\n",
      "\n",
      "\n",
      "Epoch 3/3\n",
      "\tmse : 0.185426\t\tval mse : 0.185386\tEstimated end time: 22:49:05\n",
      "\n",
      "\n",
      "\n",
      "Finished training at max epochs\n",
      "\n",
      "CPU times: user 10.7 s, sys: 444 ms, total: 11.2 s\n",
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rates = staggeredLearningRate((50,0.1),(70,0.01),(50,0.005),(50,0.001))\n",
    "printEvery=1\n",
    "%time losses,val_losses,params = train(X_train,Y_train,X_test,Y_test,rng_key,input_channels,epochs,printEvery=printEvery,learning_rates=learning_rates,params=None,forward_pass=forward_pass,tol = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 66, 8)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diego = forward_pass.apply(params,X_test[0])\n",
    "np.shape(diego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
