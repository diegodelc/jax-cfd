{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to velocities, 1st derivatives and laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_cfd.ml.diego_cnn_bcs import *\n",
    "\n",
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jax_cfd.base as cfd\n",
    "from jax_cfd.ml import towers\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "from jax_cfd.base import finite_differences as fd\n",
    "from jax_cfd.base import grids\n",
    "\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import xarray\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "# from jax_cfd.ml.diego_model_utils import SaveObject, forward_pass_module\n",
    "from jax_cfd.ml.diego_preprocessing import *\n",
    "from jax_cfd.ml.diego_train_functions import *\n",
    "from jax_cfd.ml import nonlinearities\n",
    "\n",
    "from jax_cfd.ml.newSaveObject import *\n",
    "from jax_cfd.ml.diego_towers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (fine grid)\n",
    "# create X_data via mean pooling\n",
    "# create Y_data by calculating everything for each frame and stacking them along the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '256x64_150_seconds_inner_1'\n",
    "data = xarray.open_dataset(f'../../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "high_def = []\n",
    "for i in range(int(len(data.time))):\n",
    "    this_time = np.dstack([\n",
    "        jnp.array([data.u.isel(time = i)]).reshape(x_shape,y_shape).T,\n",
    "        jnp.array([data.v.isel(time = i)]).reshape(x_shape,y_shape).T\n",
    "    ])\n",
    "    high_def.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: \t\t0.015625\n",
      "outer_steps: \t9600\n",
      "inner_steps: \t1.0\n",
      "total_sim_time: 150.0\n",
      "removed points: 960\n",
      "\n",
      "\n",
      "step = 100\n",
      "Training dataset shape: \n",
      "\t(87, 64, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "#warm up time (may want to discard initial stages of simulation since not really representative of turbulent flow?)\n",
    "dt = float(data.time[0].values)\n",
    "\n",
    "outer_steps = len(data.time.values)\n",
    "\n",
    "inner_steps = (data.time[1].values-data.time[0].values)/dt\n",
    "\n",
    "total_sim_time = outer_steps*inner_steps*dt\n",
    "print(\"dt: \\t\\t\" + str(dt))\n",
    "print(\"outer_steps: \\t\" + str(outer_steps))\n",
    "print(\"inner_steps: \\t\" + str(inner_steps))\n",
    "print(\"total_sim_time: \" + str(total_sim_time))\n",
    "\n",
    "warm_up = 15 #seconds\n",
    "warm_index = int(warm_up/total_sim_time * outer_steps // 1)\n",
    "print(\"removed points: \" + str(warm_index))\n",
    "high_def = high_def[warm_index:]\n",
    "\n",
    "print(\"\\n\")\n",
    "step = 100\n",
    "high_def = high_def[0::step]\n",
    "print(\"step = \" + str(step))\n",
    "print(\"Training dataset shape: \") # (frames, x, y, input channels)\n",
    "print(\"\\t\" + str(np.shape(high_def)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.63 ms, sys: 1.04 ms, total: 9.66 ms\n",
      "Wall time: 9.62 ms\n"
     ]
    }
   ],
   "source": [
    "%time high_def_norm,ogMean,ogStdDev = normalisingDataset(high_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining what we are training to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_outputs = {\n",
    "        \"u\" : False,\n",
    "        \"du\" : False,\n",
    "        \"lapu\" : True,\n",
    "        \n",
    "        \"v\" : False,\n",
    "        \"dv\" : False,\n",
    "        \"lapv\" : True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create X dataset: \n",
      "CPU times: user 1.93 s, sys: 5.59 ms, total: 1.94 s\n",
      "Wall time: 1.94 s\n",
      "\n",
      "Create Y dataset: \n",
      "CPU times: user 362 ms, sys: 988 µs, total: 362 ms\n",
      "Wall time: 364 ms\n",
      "\n",
      "Padding all datasets: \n",
      "CPU times: user 1.39 s, sys: 971 µs, total: 1.39 s\n",
      "Wall time: 1.39 s\n",
      "CPU times: user 4.16 s, sys: 3.78 ms, total: 4.17 s\n",
      "Wall time: 4.19 s\n",
      "CPU times: user 366 ms, sys: 976 µs, total: 367 ms\n",
      "Wall time: 369 ms\n",
      "CPU times: user 1.09 s, sys: 794 µs, total: 1.09 s\n",
      "Wall time: 1.1 s\n",
      "\n",
      "Shapes of all datasets\n",
      "(69, 18, 66, 2)\n",
      "(69, 18, 66, 6)\n",
      "(18, 18, 66, 2)\n",
      "(18, 18, 66, 6)\n"
     ]
    }
   ],
   "source": [
    "#split into train and test\n",
    "\n",
    "split = 0.8\n",
    "split = int(len(high_def)*split//1)\n",
    "random.shuffle(high_def)\n",
    "\n",
    "factor = 4\n",
    "\n",
    "print(\"Create X dataset: \")\n",
    "%time X_dataset = creatingDataset(high_def_norm,mean_pooling,factor)\n",
    "\n",
    "print(\"\\nCreate Y dataset: \")\n",
    "%time Y_dataset = createDatasetDerivatives(high_def_norm,sampling,factor,which_outputs)\n",
    "\n",
    "# %time Y_dataset = calculateResiduals(X_dataset,Y_dataset)\n",
    "\n",
    "\n",
    "X_train = X_dataset[:split]\n",
    "Y_train = Y_dataset[:split]\n",
    "\n",
    "X_test = X_dataset[split:]\n",
    "Y_test = Y_dataset[split:]\n",
    "\n",
    "# NOTE: padding conditions can be specified via the \"conditions\" input to the padYDataset function below\n",
    "# the padXDataset only pads u and v for channel flow conditions, so hard coded for impermeability and no-slip conditions (0,0)\n",
    "print(\"\\nPadding all datasets: \")\n",
    "padding = [1,1] #for a 3 by 3 kernel, find a better way to define this (so not redifined when creating CNN)\n",
    "%time X_train = padXDataset(X_train,padding)\n",
    "%time Y_train = padYDataset(Y_train,padding)\n",
    "\n",
    "%time X_test = padXDataset(X_test,padding)\n",
    "%time Y_test = padYDataset(Y_test,padding)\n",
    "\n",
    "print(\"\\nShapes of all datasets\")\n",
    "printAllShapes(X_train,Y_train, X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 8\n",
    "# spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(x):\n",
    "    cnn = CNN(CNN_specs)\n",
    "    return cnn(x)\n",
    "\n",
    "CNN_specs = {\n",
    "    \"hidden_channels\" : 4,\n",
    "    \"hidden_layers\" : 2,\n",
    "    \"nonlinearity\" : \"relu\",\n",
    "    \"num_output_channels\" : 6\n",
    "}\n",
    "\n",
    "# CNN_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of all datasets\n",
      "(69, 18, 66, 2)\n",
      "(69, 18, 66, 6)\n",
      "(18, 18, 66, 2)\n",
      "(18, 18, 66, 6)\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "\tmse : 0.121802\t\tval mse : 0.136308\tEstimated end time: 23:15:09\n",
      "\n",
      "\n",
      "Epoch 2/3\n",
      "\tmse : 0.108319\t\tval mse : 0.121205\tEstimated end time: 23:15:09\n",
      "\n",
      "\n",
      "Epoch 3/3\n",
      "\tmse : 0.097199\t\tval mse : 0.108745\tEstimated end time: 23:15:09\n",
      "\n",
      "\n",
      "\n",
      "Finished training at max epochs\n",
      "\n",
      "CPU times: user 6.03 s, sys: 82 ms, total: 6.12 s\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rates = staggeredLearningRate((120,0.01),(70,0.001))\n",
    "printEvery=1\n",
    "%time losses,val_losses,params = train(X_train,Y_train,X_test,Y_test,rng_key,input_channels,epochs,printEvery=printEvery,learning_rates=learning_rates,params=None,forward_pass=forward_pass,tol = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "plt.plot(losses[::step], label=\"training\")\n",
    "plt.plot(val_losses[::step],label=\"validation\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.xlabel(\"epochs\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toSave = newSaveObject(params,CNN_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./pred_all_eight.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,\"wb\") as f:\n",
    "    pickle.dump(toSave,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'rb',) as pickle_file:\n",
    "    loaded = pickle.load(pickle_file)\n",
    "    CNN_specs = loaded.CNN_specs\n",
    "    loaded.forward_pass = hk.without_apply_rng(hk.transform(ConvNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 66, 8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(loaded.forward_pass.apply(loaded.params,X_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
