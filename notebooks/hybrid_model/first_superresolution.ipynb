{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a superresolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad\n",
    "from jax_cfd.ml import towers\n",
    "import haiku as hk\n",
    "import gin\n",
    "import numpy as np\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "import xarray \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '256x64_inner_50_outer_1000'\n",
    "data = xarray.open_dataset(f'../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "high_def = []\n",
    "for i in range(len(data.time)):\n",
    "    this_time_u = np.array([data.u.isel(time = i)]).reshape(x_shape, y_shape)\n",
    "    this_time_v = np.array([data.v.isel(time = i)]).reshape(x_shape, y_shape)\n",
    "    this_time = [this_time_u, this_time_v]\n",
    "    high_def.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2, 256, 64)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(high_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: \t\t0.78125\n",
      "outer_steps: \t1000\n",
      "inner_steps: \t1.0\n",
      "total_sim_time: 781.25\n",
      "removed points: 38\n"
     ]
    }
   ],
   "source": [
    "#warm up time (may want to discard initial stages of simulation since not really representative of turbulent flow?)\n",
    "dt = float(data.time[0].values)\n",
    "\n",
    "outer_steps = len(data.time.values)\n",
    "\n",
    "inner_steps = (data.time[1].values-data.time[0].values)/dt\n",
    "\n",
    "total_sim_time = outer_steps*inner_steps*dt\n",
    "print(\"dt: \\t\\t\" + str(dt))\n",
    "print(\"outer_steps: \\t\" + str(outer_steps))\n",
    "print(\"inner_steps: \\t\" + str(inner_steps))\n",
    "print(\"total_sim_time: \" + str(total_sim_time))\n",
    "\n",
    "warm_up = 30 #seconds\n",
    "warm_index = int(warm_up/total_sim_time * outer_steps // 1)\n",
    "print(\"removed points: \" + str(warm_index))\n",
    "high_def = high_def[warm_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise velocities to beetween zero and one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([[1,2,3], [3,4,3], [5,6,3]])\n",
    "# a = np.random.rand((9*9)).reshape(9,9) * 10 //1\n",
    "# print(a)\n",
    "\n",
    "\n",
    "\n",
    "def increaseSize(input, factor):\n",
    "    w,h = np.shape(input)\n",
    "    output = np.zeros((w*factor,h*factor))\n",
    "    \n",
    "    for width in range(w*factor):\n",
    "        for height in range(h*factor):\n",
    "            output[width][height] = input[width//factor][height//factor]\n",
    "    return output\n",
    "\n",
    "\n",
    "def decreaseSize(input,factor):\n",
    "    w,h = np.shape(input)\n",
    "    if w%factor != 0 or h%factor != 0:\n",
    "        raise(AssertionError(\"Non-compatible input shape and downsample factor\"))\n",
    "    \n",
    "    output = np.zeros((int(w/factor),int(h/factor)))\n",
    "    \n",
    "    for width in range(w):\n",
    "        for height in range(h):\n",
    "            output[width//factor][height//factor] += input[width][height]\n",
    "    output /= factor**len(np.shape(output))\n",
    "    return output\n",
    "\n",
    "def downsampleHighDefVels(high_def,factor):\n",
    "    low_def = []\n",
    "    for vels in high_def:\n",
    "        both_vels = []\n",
    "        for vel in vels:\n",
    "            vel = decreaseSize(vel,factor)\n",
    "\n",
    "            vel = increaseSize(vel,factor)\n",
    "            both_vels.append(vel)\n",
    "        low_def.append(both_vels)\n",
    "    return low_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "\n",
    "split = 0.8\n",
    "split = int(len(high_def)*split//1)\n",
    "random.shuffle(high_def)\n",
    "\n",
    "factor = 2\n",
    "low_def = downsampleHighDefVels(high_def,factor)\n",
    "\n",
    "X_train = low_def[:split]\n",
    "Y_train = high_def[:split]\n",
    "\n",
    "X_test = low_def[split:]\n",
    "Y_test = high_def[split:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007942801341414452"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = 200\n",
    "vel = 1\n",
    "mse(X_train[time][vel],Y_train[time+1][vel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference:\n",
    "# https://goodboychan.github.io/python/deep_learning/vision/tensorflow-keras/2020/10/13/01-Super-Resolution-CNN.html#Build-SR-CNN-Model\n",
    "\n",
    "def mse(target, ref):\n",
    "    target_data = target.astype(np.float32)\n",
    "    ref_data = ref.astype(np.float32)\n",
    "    err = np.sum((target_data - ref_data) ** 2)\n",
    "    \n",
    "    err /= float(target_data.shape[0] * target_data.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_module(\n",
    "    num_output_channels,\n",
    "    ndim,\n",
    "    tower_module=gin.REQUIRED\n",
    "):\n",
    "  \"\"\"Constructs a function that initializes tower and applies it to inputs.\"\"\"\n",
    "  def forward_pass(inputs):\n",
    "    return tower_module(num_output_channels, ndim)(inputs)\n",
    "\n",
    "  return forward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 2\n",
    "spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "inputs = jax.random.uniform(rng, (spatial_size,) * ndim + (input_channels,))\n",
    "\n",
    "forward_pass = hk.without_apply_rng(\n",
    "                    hk.transform(\n",
    "                        forward_pass_module(num_output_channels = num_output_channels, \n",
    "                                            ndim = ndim,\n",
    "                                           tower_module = towers.forward_tower_factory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients\n\u001b[1;32m     15\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 16\u001b[0m params \u001b[38;5;241m=\u001b[39m forward_pass\u001b[38;5;241m.\u001b[39minit(rng, \u001b[43mX_train\u001b[49m[:batch_size])\n\u001b[1;32m     17\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     18\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/haiku-guide-to-create-multi-layer-perceptrons-using-jax\n",
    "\n",
    "# define X_train and Y_train\n",
    "\n",
    "def MeanSquaredErrorLoss(weights, input_data, actual):\n",
    "    preds = model.apply(weights, rng, input_data)\n",
    "    preds = preds.squeeze()\n",
    "    return jnp.power(actual - preds, 2).mean()\n",
    "\n",
    "def UpdateWeights(weights,gradients):\n",
    "    return weights - learning_rate * gradients\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "params = forward_pass.init(rng, X_train[:batch_size])\n",
    "epochs = 1000\n",
    "learning_rate = jnp.array(0.001)\n",
    "\n",
    "\n",
    "def train_step(params, X_train, Y_train):\n",
    "    loss, param_grads = value_and_grad(MeanSquaredErrorLoss)(params, X_train, Y_train)\n",
    "    return jax.tree_map(UpdateWeights, params, param_grads), loss\n",
    "\n",
    "train_step = jax.jit(train_step)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    params,loss = train_step(params, X_train, Y_train)\n",
    "\n",
    "    if i%100 == 0: #every hundred epochs\n",
    "        print(\"MSE : {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_utils.loss_and_gradient\n",
    "# train_utils.train_step(\n",
    "#     loss_and_grad_fn= train_utils.loss_and_gradient,\n",
    "#     update_fn =  Callable[[int, ModelGradients, OptimizerState], OptimizerState],\n",
    "#     get_params_fn = Callable[[OptimizerState], ModelParams]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params = forward_pass.init(rng, inputs)\n",
    "# output = forward_pass.apply(params, inputs)\n",
    "# expected_output_shape = inputs.shape[:-1] + (num_output_channels,)\n",
    "# actual_output_shape = output.shape\n",
    "\n",
    "# print(expected_output_shape,actual_output_shape)\n",
    "# mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(trainable_params, non_trainable_params, images, labels):\n",
    "#   # NOTE: We need to combine trainable and non trainable before calling apply.\n",
    "#   params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "\n",
    "#   # NOTE: From here on this is a standard softmax cross entropy loss.\n",
    "#   logits = f.apply(params, None, images)\n",
    "#   labels = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "#   return -jnp.sum(labels * jax.nn.log_softmax(logits)) / labels.shape[0]\n",
    "\n",
    "# def sgd_step(params, grads, *, lr):\n",
    "#   return jax.tree_util.tree_map(lambda p, g: p - g * lr, params, grads)\n",
    "\n",
    "# def train_step(trainable_params, non_trainable_params, x, y):\n",
    "#   # NOTE: We will only compute gradients wrt `trainable_params`.\n",
    "#   trainable_params_grads = jax.grad(loss_fn)(trainable_params,\n",
    "#                                              non_trainable_params, x, y)\n",
    "\n",
    "#   # NOTE: We are only updating `trainable_params`.\n",
    "#   trainable_params = sgd_step(trainable_params, trainable_params_grads, lr=0.1)\n",
    "#   return trainable_params\n",
    "\n",
    "# train_step = jax.jit(train_step)\n",
    "\n",
    "# for x, y in dataset(batch_size=num_classes, num_records=10000):\n",
    "#   # NOTE: In our training loop only our trainable parameters are updated.\n",
    "#   trainable_params = train_step(trainable_params, non_trainable_params, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
