{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a superresolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad\n",
    "from jax_cfd.ml import towers\n",
    "import haiku as hk\n",
    "import gin\n",
    "import numpy as np\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "import xarray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '1080x256_inner_1_outer_1000'\n",
    "data = xarray.open_dataset(f'../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "ground_truth = []\n",
    "for i in range(len(data.time)):\n",
    "    this_time = np.array(data.u.isel(time = i), data.v.isel(time= i)).reshape(x_shape, y_shape)\n",
    "    ground_truth.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create downsampled images (same size as large or smaller???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into X and Y (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference:\n",
    "# https://goodboychan.github.io/python/deep_learning/vision/tensorflow-keras/2020/10/13/01-Super-Resolution-CNN.html#Build-SR-CNN-Model\n",
    "\n",
    "def mse(target, ref):\n",
    "    target_data = target.astype(np.float32)\n",
    "    ref_data = ref.astype(np.float32)\n",
    "    err = np.sum((target_data - ref_data) ** 2)\n",
    "    \n",
    "    err /= float(target_data.shape[0] * target_data.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_module(\n",
    "    num_output_channels,\n",
    "    ndim,\n",
    "    tower_module=gin.REQUIRED\n",
    "):\n",
    "  \"\"\"Constructs a function that initializes tower and applies it to inputs.\"\"\"\n",
    "  def forward_pass(inputs):\n",
    "    return tower_module(num_output_channels, ndim)(inputs)\n",
    "\n",
    "  return forward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 2\n",
    "spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "inputs = jax.random.uniform(rng, (spatial_size,) * ndim + (input_channels,))\n",
    "\n",
    "forward_pass = hk.without_apply_rng(\n",
    "                    hk.transform(\n",
    "                        forward_pass_module(num_output_channels = num_output_channels, \n",
    "                                            ndim = ndim,\n",
    "                                           tower_module = towers.forward_tower_factory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients\n\u001b[1;32m     15\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 16\u001b[0m params \u001b[38;5;241m=\u001b[39m forward_pass\u001b[38;5;241m.\u001b[39minit(rng, \u001b[43mX_train\u001b[49m[:batch_size])\n\u001b[1;32m     17\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     18\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/haiku-guide-to-create-multi-layer-perceptrons-using-jax\n",
    "\n",
    "# define X_train and Y_train\n",
    "\n",
    "def MeanSquaredErrorLoss(weights, input_data, actual):\n",
    "    preds = model.apply(weights, rng, input_data)\n",
    "    preds = preds.squeeze()\n",
    "    return jnp.power(actual - preds, 2).mean()\n",
    "\n",
    "def UpdateWeights(weights,gradients):\n",
    "    return weights - learning_rate * gradients\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "params = forward_pass.init(rng, X_train[:batch_size])\n",
    "epochs = 1000\n",
    "learning_rate = jnp.array(0.001)\n",
    "\n",
    "\n",
    "def train_step(params, X_train, Y_train):\n",
    "    loss, param_grads = value_and_grad(MeanSquaredErrorLoss)(params, X_train, Y_train)\n",
    "    return jax.tree_map(UpdateWeights, params, param_grads), loss\n",
    "\n",
    "train_step = jax.jit(train_step)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    params,loss = train_step(params, X_train, Y_train)\n",
    "\n",
    "    if i%100 == 0: #every hundred epochs\n",
    "        print(\"MSE : {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_utils.loss_and_gradient\n",
    "# train_utils.train_step(\n",
    "#     loss_and_grad_fn= train_utils.loss_and_gradient,\n",
    "#     update_fn =  Callable[[int, ModelGradients, OptimizerState], OptimizerState],\n",
    "#     get_params_fn = Callable[[OptimizerState], ModelParams]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params = forward_pass.init(rng, inputs)\n",
    "# output = forward_pass.apply(params, inputs)\n",
    "# expected_output_shape = inputs.shape[:-1] + (num_output_channels,)\n",
    "# actual_output_shape = output.shape\n",
    "\n",
    "# print(expected_output_shape,actual_output_shape)\n",
    "# mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(trainable_params, non_trainable_params, images, labels):\n",
    "#   # NOTE: We need to combine trainable and non trainable before calling apply.\n",
    "#   params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "\n",
    "#   # NOTE: From here on this is a standard softmax cross entropy loss.\n",
    "#   logits = f.apply(params, None, images)\n",
    "#   labels = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "#   return -jnp.sum(labels * jax.nn.log_softmax(logits)) / labels.shape[0]\n",
    "\n",
    "# def sgd_step(params, grads, *, lr):\n",
    "#   return jax.tree_util.tree_map(lambda p, g: p - g * lr, params, grads)\n",
    "\n",
    "# def train_step(trainable_params, non_trainable_params, x, y):\n",
    "#   # NOTE: We will only compute gradients wrt `trainable_params`.\n",
    "#   trainable_params_grads = jax.grad(loss_fn)(trainable_params,\n",
    "#                                              non_trainable_params, x, y)\n",
    "\n",
    "#   # NOTE: We are only updating `trainable_params`.\n",
    "#   trainable_params = sgd_step(trainable_params, trainable_params_grads, lr=0.1)\n",
    "#   return trainable_params\n",
    "\n",
    "# train_step = jax.jit(train_step)\n",
    "\n",
    "# for x, y in dataset(batch_size=num_classes, num_records=10000):\n",
    "#   # NOTE: In our training loop only our trainable parameters are updated.\n",
    "#   trainable_params = train_step(trainable_params, non_trainable_params, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
