{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a superresolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad\n",
    "from jax_cfd.ml import towers\n",
    "import haiku as hk\n",
    "import gin\n",
    "import numpy as np\n",
    "import jax_cfd.ml.train_utils as train_utils\n",
    "import xarray \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "file_name = '256x64_inner_50_outer_1000'\n",
    "data = xarray.open_dataset(f'../creating_dataset/datasets/'+ file_name +'.nc', chunks={'time': '100MB'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by timestamps\n",
    "x_shape = len(data.x)\n",
    "y_shape = len(data.y)\n",
    "high_def = []\n",
    "for i in range(len(data.time)):\n",
    "    this_time_u = np.array([data.u.isel(time = i)]).reshape(x_shape, y_shape)\n",
    "    this_time_v = np.array([data.v.isel(time = i)]).reshape(x_shape, y_shape)\n",
    "    this_time = [this_time_u, this_time_v]\n",
    "    high_def.append(this_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: \t\t0.78125\n",
      "outer_steps: \t1000\n",
      "inner_steps: \t1.0\n",
      "total_sim_time: 781.25\n",
      "removed points: 38\n"
     ]
    }
   ],
   "source": [
    "#warm up time (may want to discard initial stages of simulation since not really representative of turbulent flow?)\n",
    "dt = float(data.time[0].values)\n",
    "\n",
    "outer_steps = len(data.time.values)\n",
    "\n",
    "inner_steps = (data.time[1].values-data.time[0].values)/dt\n",
    "\n",
    "total_sim_time = outer_steps*inner_steps*dt\n",
    "print(\"dt: \\t\\t\" + str(dt))\n",
    "print(\"outer_steps: \\t\" + str(outer_steps))\n",
    "print(\"inner_steps: \\t\" + str(inner_steps))\n",
    "print(\"total_sim_time: \" + str(total_sim_time))\n",
    "\n",
    "warm_up = 30 #seconds\n",
    "warm_index = int(warm_up/total_sim_time * outer_steps // 1)\n",
    "print(\"removed points: \" + str(warm_index))\n",
    "high_def = high_def[warm_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.74617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(high_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalMax = np.max(high_def)\n",
    "originalMin = (np.min(high_def))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.78758466 1.74617\n"
     ]
    }
   ],
   "source": [
    "print(originalMin,originalMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale velocities to beetween zero and one\n",
    "def scale(input, min=0, max=1):\n",
    "    \n",
    "    if min>max:\n",
    "        raise(ValueError(\"Min and max may be the wrong way around\"))\n",
    "    \n",
    "    if min<0:\n",
    "        og_min = min\n",
    "        max += -min\n",
    "        min = 0\n",
    "\n",
    "    \n",
    "    input += (min-np.min(input))\n",
    "    input /= (np.max(input)/max)\n",
    "    \n",
    "    try:\n",
    "        input += og_min\n",
    "    except:\n",
    "        pass\n",
    "    return input\n",
    "\n",
    "\n",
    "# this function is useless, it does the same as scale()\n",
    "# def scaleBack(input,originalMin,originalMax):\n",
    "#     inputMin = np.min(input)\n",
    "#     inputMax = np.max(input)\n",
    "#     inputRange = inputMax-inputMin\n",
    "    \n",
    "#     originalRange = originalMax-originalMin\n",
    "    \n",
    "#     input += inputMin #sets min to zero\n",
    "#     input *= originalRange/inputRange\n",
    "#     input -= originalMin\n",
    "    \n",
    "#     return input\n",
    "\n",
    "\n",
    "def scaleAllVelocities(high_def,allVels=False):\n",
    "    \n",
    "    scaled = []\n",
    "    for vels in high_def:\n",
    "        both_vels = []\n",
    "        for vel in vels:\n",
    "            vel = scale(vel,min=0,max=1)\n",
    "            both_vels.append(vel)\n",
    "        scaled.append(both_vels)\n",
    "    return scaled\n",
    "        \n",
    "\n",
    "# scaled_high_def = scaleAllVelocities(high_def,allVels=True) #scales each frame from 0 to 1\n",
    "scaled_high_def = scale(high_def) #scales all timesteps to same max and min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increaseSize(input, factor):\n",
    "    w,h = np.shape(input)\n",
    "    output = np.zeros((w*factor,h*factor))\n",
    "    \n",
    "    for width in range(w*factor):\n",
    "        for height in range(h*factor):\n",
    "            output[width][height] = input[width//factor][height//factor]\n",
    "    return output\n",
    "\n",
    "\n",
    "def decreaseSize(input,factor):\n",
    "    w,h = np.shape(input)\n",
    "    if w%factor != 0 or h%factor != 0:\n",
    "        raise(AssertionError(\"Non-compatible input shape and downsample factor\"))\n",
    "    \n",
    "    output = np.zeros((int(w/factor),int(h/factor)))\n",
    "    \n",
    "    for width in range(w):\n",
    "        for height in range(h):\n",
    "            output[width//factor][height//factor] += input[width][height]\n",
    "    output /= factor**len(np.shape(output))\n",
    "    return output\n",
    "\n",
    "def downsampleHighDefVels(high_def,factor):\n",
    "    low_def = []\n",
    "    for vels in high_def:\n",
    "        both_vels = []\n",
    "        for vel in vels:\n",
    "            vel = decreaseSize(vel,factor)\n",
    "\n",
    "            vel = increaseSize(vel,factor)\n",
    "            both_vels.append(vel)\n",
    "        low_def.append(both_vels)\n",
    "    return low_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting lists of images to DeviceArray with correct shape\n",
    "\n",
    "def convertListToDeviceArray(list):\n",
    "    new_list = []\n",
    "    for i in range(len(list)):\n",
    "\n",
    "        new_list.append(\n",
    "            jnp.dstack([\n",
    "                jnp.array(list[i][0]),\n",
    "                jnp.array(list[i][1])]\n",
    "            )\n",
    "        )\n",
    "    return jnp.array(new_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 s, sys: 166 ms, total: 33.9 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "#split into train and test\n",
    "\n",
    "split = 0.8\n",
    "split = int(len(scaled_high_def)*split//1)\n",
    "random.shuffle(high_def)\n",
    "\n",
    "factor = 2\n",
    "\n",
    "\n",
    "\n",
    "%time scaled_low_def = scale(downsampleHighDefVels(scaled_high_def,factor))\n",
    "scaled_high_def = convertListToDeviceArray(scaled_high_def)\n",
    "scaled_low_def = convertListToDeviceArray(scaled_low_def)\n",
    "\n",
    "X_train = scaled_low_def[:split]\n",
    "Y_train = scaled_high_def[:split]\n",
    "\n",
    "X_test = scaled_low_def[split:]\n",
    "Y_test = scaled_high_def[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 256, 64, 2)\n",
      "(962, 256, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(scaled_high_def))\n",
    "print(np.shape(scaled_low_def))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference:\n",
    "# https://goodboychan.github.io/python/deep_learning/vision/tensorflow-keras/2020/10/13/01-Super-Resolution-CNN.html#Build-SR-CNN-Model\n",
    "\n",
    "def mse(target, ref):\n",
    "    target_data = target.astype(np.float32)\n",
    "    ref_data = ref.astype(np.float32)\n",
    "    err = np.sum((target_data - ref_data) ** 2)\n",
    "    \n",
    "    err /= float(target_data.shape[0] * target_data.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_module(\n",
    "    num_output_channels,\n",
    "    ndim,\n",
    "    tower_module=gin.REQUIRED\n",
    "):\n",
    "  \"\"\"Constructs a function that initializes tower and applies it to inputs.\"\"\"\n",
    "  def forward_pass(inputs):\n",
    "    return tower_module(num_output_channels, ndim)(inputs)\n",
    "\n",
    "  return forward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 2\n",
    "# spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "# inputs = jax.random.uniform(rng, (spatial_size,) * ndim + (input_channels,))\n",
    "\n",
    "forward_pass = hk.without_apply_rng(hk.transform(forward_pass_module(num_output_channels = num_output_channels, \n",
    "                                            ndim = ndim,\n",
    "                                           tower_module = towers.forward_tower_factory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "diego = [1,1,2,3]\n",
    "print(type(diego))\n",
    "diego = np.array(diego)\n",
    "print(type(diego))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got this far\n",
      "Epoch 1/100\n",
      "\tMSE : 0.161420\n",
      "Epoch 2/100\n",
      "\tMSE : 0.084578\n",
      "Epoch 3/100\n",
      "\tMSE : 0.024321\n",
      "Epoch 4/100\n",
      "\tMSE : 0.006604\n",
      "Epoch 5/100\n",
      "\tMSE : 0.006146\n",
      "Epoch 6/100\n",
      "\tMSE : 0.005805\n",
      "Epoch 7/100\n",
      "\tMSE : 0.005492\n",
      "Epoch 8/100\n",
      "\tMSE : 0.005200\n"
     ]
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/haiku-guide-to-create-multi-layer-perceptrons-using-jax\n",
    "\n",
    "# define X_train and Y_train\n",
    "\n",
    "def MeanSquaredErrorLoss(params, input_data, actual):\n",
    "#     print(np.shape(input_data[0]))\n",
    "#     params = forward_pass.init(rng, inputs)\n",
    "    preds = []\n",
    "    truth = []\n",
    "    for i in range(len(input_data)):\n",
    "        preds.append(forward_pass.apply(params, input_data[i]))\n",
    "        truth.append(actual[i])\n",
    "    \n",
    "    \n",
    "#     preds = forward_pass.apply(params, x=input_data, rng = None)\n",
    "#     preds = preds.squeeze()\n",
    "    return jnp.power(jnp.array(truth) - jnp.array(preds), 2).mean()\n",
    "\n",
    "def UpdateWeights(weights,gradients):\n",
    "    return weights - learning_rate * gradients\n",
    "\n",
    "# X_train[:batch_size]\n",
    "\n",
    "sample_x = jax.random.uniform(rng_key, (256,64,input_channels))\n",
    "batch_size = 1\n",
    "params = forward_pass.init(rng_key, sample_x)\n",
    "epochs = 100\n",
    "learning_rate = jnp.array(0.1)\n",
    "\n",
    "\n",
    "def train_step(params, X_train, Y_train):\n",
    "    loss, param_grads = value_and_grad(MeanSquaredErrorLoss)(params, X_train, Y_train)\n",
    "    return jax.tree_map(UpdateWeights, params, param_grads), loss\n",
    "\n",
    "# train_step = jax.jit(train_step)\n",
    "\n",
    "print(\"got this far\")\n",
    "\n",
    "losses = []\n",
    "for i in range(1, epochs+1):\n",
    "    params,loss = train_step(params, X_train, Y_train)\n",
    "\n",
    "    if i%1 == 0: #every 5 epochs\n",
    "        print(\"Epoch {:.0f}/{:.0f}\".format(i,epochs))\n",
    "        print(\"\\tMSE : {:.6f}\".format(loss))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "\n",
    "input_data = X_train[:num]\n",
    "actual = Y_train[:num]\n",
    "\n",
    "preds = []\n",
    "truth = []\n",
    "for i in range(len(input_data)):\n",
    "    preds.append(forward_pass.apply(params, input_data[i]))\n",
    "    truth.append(actual[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.power(np.array(truth) - np.array(preds), 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = forward_pass.apply(params,scaled_low_def[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 64, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(preds))\n",
    "np.shape(scaled_high_def[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = jax.random.uniform(rng, (256,64,input_channels))\n",
    "np.shape(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 64, 2)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(preds.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 2)\n",
      "(256, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(preds))\n",
    "print(np.shape(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.09285066, dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.power(Y_train[0] - preds, 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_utils.loss_and_gradient\n",
    "# train_utils.train_step(\n",
    "#     loss_and_grad_fn= train_utils.loss_and_gradient,\n",
    "#     update_fn =  Callable[[int, ModelGradients, OptimizerState], OptimizerState],\n",
    "#     get_params_fn = Callable[[OptimizerState], ModelParams]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params = forward_pass.init(rng, inputs)\n",
    "# output = forward_pass.apply(params, inputs)\n",
    "# expected_output_shape = inputs.shape[:-1] + (num_output_channels,)\n",
    "# actual_output_shape = output.shape\n",
    "\n",
    "# print(expected_output_shape,actual_output_shape)\n",
    "# mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(trainable_params, non_trainable_params, images, labels):\n",
    "#   # NOTE: We need to combine trainable and non trainable before calling apply.\n",
    "#   params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "\n",
    "#   # NOTE: From here on this is a standard softmax cross entropy loss.\n",
    "#   logits = f.apply(params, None, images)\n",
    "#   labels = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "#   return -jnp.sum(labels * jax.nn.log_softmax(logits)) / labels.shape[0]\n",
    "\n",
    "# def sgd_step(params, grads, *, lr):\n",
    "#   return jax.tree_util.tree_map(lambda p, g: p - g * lr, params, grads)\n",
    "\n",
    "# def train_step(trainable_params, non_trainable_params, x, y):\n",
    "#   # NOTE: We will only compute gradients wrt `trainable_params`.\n",
    "#   trainable_params_grads = jax.grad(loss_fn)(trainable_params,\n",
    "#                                              non_trainable_params, x, y)\n",
    "\n",
    "#   # NOTE: We are only updating `trainable_params`.\n",
    "#   trainable_params = sgd_step(trainable_params, trainable_params_grads, lr=0.1)\n",
    "#   return trainable_params\n",
    "\n",
    "# train_step = jax.jit(train_step)\n",
    "\n",
    "# for x, y in dataset(batch_size=num_classes, num_records=10000):\n",
    "#   # NOTE: In our training loop only our trainable parameters are updated.\n",
    "#   trainable_params = train_step(trainable_params, non_trainable_params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
