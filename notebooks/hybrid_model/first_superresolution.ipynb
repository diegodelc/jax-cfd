{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a superresolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad\n",
    "from jax_cfd.ml import towers\n",
    "import haiku as hk\n",
    "import gin\n",
    "import numpy as np\n",
    "import jax_cfd.ml.train_utils as train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference:\n",
    "# https://goodboychan.github.io/python/deep_learning/vision/tensorflow-keras/2020/10/13/01-Super-Resolution-CNN.html#Build-SR-CNN-Model\n",
    "\n",
    "def mse(target, ref):\n",
    "    target_data = target.astype(np.float32)\n",
    "    ref_data = ref.astype(np.float32)\n",
    "    err = np.sum((target_data - ref_data) ** 2)\n",
    "    \n",
    "    err /= float(target_data.shape[0] * target_data.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_pass_module(\n",
    "    num_output_channels,\n",
    "    ndim,\n",
    "    tower_module=gin.REQUIRED\n",
    "):\n",
    "  \"\"\"Constructs a function that initializes tower and applies it to inputs.\"\"\"\n",
    "  def forward_pass(inputs):\n",
    "    return tower_module(num_output_channels, ndim)(inputs)\n",
    "\n",
    "  return forward_pass\n",
    "\n",
    "f = forward_pass_module(num_output_channels = 2,\n",
    "                        ndim = 2,\n",
    "                        tower_module = towers.forward_tower_factory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.random.rand(8,28,28)\n",
    "# # x = jnp.ones([8,28,28]) # simulates 8 28x28 images\n",
    "# print(jnp.shape(x))\n",
    "# print(hk.experimental.tabulate(f)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_channels = 2\n",
    "spatial_size = 17\n",
    "ndim = 2\n",
    "input_channels = 2\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "inputs = jax.random.uniform(rng, (spatial_size,) * ndim + (input_channels,))\n",
    "\n",
    "forward_pass = hk.without_apply_rng(\n",
    "                    hk.transform(\n",
    "                        forward_pass_module(num_output_channels = num_output_channels, \n",
    "                                            ndim = ndim,\n",
    "                                           tower_module = towers.forward_tower_factory)))\n",
    "params = forward_pass.init(rng, inputs)\n",
    "output = forward_pass.apply(params, inputs)\n",
    "expected_output_shape = inputs.shape[:-1] + (num_output_channels,)\n",
    "actual_output_shape = output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 17, 2) (17, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "print(expected_output_shape,actual_output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.2741005, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m trainable_params\n\u001b[1;32m     22\u001b[0m train_step \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(train_step)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m(batch_size\u001b[38;5;241m=\u001b[39mnum_classes, num_records\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;66;03m# NOTE: In our training loop only our trainable parameters are updated.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m   trainable_params \u001b[38;5;241m=\u001b[39m train_step(trainable_params, non_trainable_params, x, y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def loss_fn(trainable_params, non_trainable_params, images, labels):\n",
    "  # NOTE: We need to combine trainable and non trainable before calling apply.\n",
    "  params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "\n",
    "  # NOTE: From here on this is a standard softmax cross entropy loss.\n",
    "  logits = f.apply(params, None, images)\n",
    "  labels = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "  return -jnp.sum(labels * jax.nn.log_softmax(logits)) / labels.shape[0]\n",
    "\n",
    "def sgd_step(params, grads, *, lr):\n",
    "  return jax.tree_util.tree_map(lambda p, g: p - g * lr, params, grads)\n",
    "\n",
    "def train_step(trainable_params, non_trainable_params, x, y):\n",
    "  # NOTE: We will only compute gradients wrt `trainable_params`.\n",
    "  trainable_params_grads = jax.grad(loss_fn)(trainable_params,\n",
    "                                             non_trainable_params, x, y)\n",
    "\n",
    "  # NOTE: We are only updating `trainable_params`.\n",
    "  trainable_params = sgd_step(trainable_params, trainable_params_grads, lr=0.1)\n",
    "  return trainable_params\n",
    "\n",
    "train_step = jax.jit(train_step)\n",
    "\n",
    "for x, y in dataset(batch_size=num_classes, num_records=10000):\n",
    "  # NOTE: In our training loop only our trainable parameters are updated.\n",
    "  trainable_params = train_step(trainable_params, non_trainable_params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients\n\u001b[1;32m     15\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 16\u001b[0m params \u001b[38;5;241m=\u001b[39m forward_pass\u001b[38;5;241m.\u001b[39minit(rng, \u001b[43mX_train\u001b[49m[:batch_size])\n\u001b[1;32m     17\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     18\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/haiku-guide-to-create-multi-layer-perceptrons-using-jax\n",
    "\n",
    "# define X_train and Y_train\n",
    "\n",
    "def MeanSquaredErrorLoss(weights, input_data, actual):\n",
    "    preds = model.apply(weights, rng, input_data)\n",
    "    preds = preds.squeeze()\n",
    "    return jnp.power(actual - preds, 2).mean()\n",
    "\n",
    "def UpdateWeights(weights,gradients):\n",
    "    return weights - learning_rate * gradients\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "params = forward_pass.init(rng, X_train[:batch_size])\n",
    "epochs = 1000\n",
    "learning_rate = jnp.array(0.001)\n",
    "\n",
    "\n",
    "def train_step(params, X_train, Y_train):\n",
    "    loss, param_grads = value_and_grad(MeanSquaredErrorLoss)(params, X_train, Y_train)\n",
    "    return jax.tree_map(UpdateWeights, params, param_grads), loss\n",
    "\n",
    "train_step = jax.jit(train_step)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    params,loss = train_step(params, X_train, Y_train)\n",
    "\n",
    "    if i%100 == 0: #every hundred epochs\n",
    "        print(\"MSE : {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_utils.loss_and_gradient\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_utils\u001b[38;5;241m.\u001b[39mtrain_step(\n\u001b[1;32m      3\u001b[0m     loss_and_grad_fn\u001b[38;5;241m=\u001b[39m train_utils\u001b[38;5;241m.\u001b[39mloss_and_gradient,\n\u001b[0;32m----> 4\u001b[0m     update_fn \u001b[38;5;241m=\u001b[39m  \u001b[43mCallable\u001b[49m[[\u001b[38;5;28mint\u001b[39m, ModelGradients, OptimizerState], OptimizerState],\n\u001b[1;32m      5\u001b[0m     get_params_fn \u001b[38;5;241m=\u001b[39m Callable[[OptimizerState], ModelParams]\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Callable' is not defined"
     ]
    }
   ],
   "source": [
    "# train_utils.loss_and_gradient\n",
    "train_utils.train_step(\n",
    "    loss_and_grad_fn= train_utils.loss_and_gradient,\n",
    "    update_fn =  Callable[[int, ModelGradients, OptimizerState], OptimizerState],\n",
    "    get_params_fn = Callable[[OptimizerState], ModelParams]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PNAS_codes]",
   "language": "python",
   "name": "conda-env-PNAS_codes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
